---
title: "Web Scraping"
author: "Matthew Ross"
date: "10/20/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)
library(pdftools)

```


# Simple web scraping

R can read html using either rvest, xml, or xml2 packages. Here
we are going to navigate to the Center for Snow and Avalance Studies 
[Website](https://snowstudies.org/archived-data/) and read a table in.
This table contains links to data we want to programatically download for
three sites. I don't know much about these sites, but they contain incredibly
rich snow, temperature, and precip data. 


## Reading an html 

### Extract csv links from webpage


```{r}
site_url <- 'https://snowstudies.org/archived-data/'

#Read the web url
webpage <- read_html(site_url)

#See if we can extract tables and get the data that way
tables <- webpage %>%
  html_nodes('table') %>%
  magrittr::extract2(3) %>%
  html_table(fill = TRUE)
#That didn't work, so let's try a different approach

#Extract only weblinks and then the URLs!
links <- webpage %>%
  html_nodes('a') %>%
  .[grepl('24 Hour',.)] %>%
  html_attr('href')


```

## Data Download

### Download data in a for loop

```{r}

#Grab only the name of the file by splitting out on forward slashes
splits <- str_split_fixed(links,'/',8)

#Keep only the 8th column
dataset <- splits[,8] 

#generate a file list for where the data goes
file_names <- paste0('data/',dataset)

# for(i in 1:length(file_names)){
#   download.file(links[i],destfile=file_names[i])
# }

```

### Download data in a map

```{r}

#Map version of the same forloop (downloading 4 files)
map2(links,file_names,download.file)

```

## Data read-in 

### Read in just the snow data as a loop

```{r}
#Pattern matching to only keep certain files
snow_files <- file_names %>%
  .[!grepl('SG_24',.)] %>%
  .[!grepl('PTSP',.)]

#empty_data <- list()

# snow_data <- for(i in 1:length(snow_files)){
#   empty_data[[i]] <- read_csv(snow_files[i]) %>%
#     select(Year,DOY,Sno_Height_M)
# }


#snow_data_full <- do.call('rbind',empty_data)

#summary(snow_data_full)
```


### Read in the data as a map function

```{r}


our_snow_reader <- function(file){
  name = str_split_fixed(file,'/',2)[,2] %>%
    gsub('_24hr.csv','',.)
  df <- read_csv(file) %>%
    select(Year,DOY,Sno_Height_M) %>%
    mutate(site = name)
}


snow_data_full <- map_dfr(snow_files,our_snow_reader)

summary(snow_data_full)
```


### Plot snow data

```{r}
snow_yearly <- snow_data_full %>%
  group_by(Year,site) %>%
  summarize(mean_height = mean(Sno_Height_M,na.rm=T))


ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + 
  geom_point() +
  ggthemes::theme_few() + 
  ggthemes::scale_color_few()
```


# In-Class work

## Extracting meteorological data urls

Here I want you to use the `rvest` package to get the URLs for
the `SASP forcing` and `SBSP_forcing` meteoroligical datasets.

```{r}

links_met <- webpage %>%
  html_nodes('a') %>%
  .[grepl('forcing',.)] %>%
  html_attr('href')

links_met
```

## Download the meteorological data

Here I want you to use the `download_file` and `str_split_fixed` 
commands to download the data ands save it in your data folder.
You can use a for loop or a map function. 

```{r}

###split file names along /
splits_met <- str_split_fixed(links_met,'/',5)
splits_met

###split file names along -
splits_met <- str_split_fixed(splits_met[,5], "-", 4)
splits_met

###Keep only 3rd column of split along / and -
dataset_met <- splits_met[, 3]
dataset_met


###Create files
file_names_met <- paste0("data/", dataset_met, "_metdata")
file_names_met


###Read in and store data
map2(links_met,file_names_met,download.file)



```

## Read in the data

Write a custom function to read in the data and append a site 
column to the data. 

```{r}

###Read in met data
met_files <- file_names_met %>%
  .[grepl('_metdata',.)] 

met_file1 <- read_delim(met_files[1], delim = " ", col_names = FALSE)

met_metadata <- webpage %>%
  html_nodes('a') %>%
  .[grepl('Read Me',.)] %>%
  html_attr('href')

met_metadata_text <- pdf_text(met_metadata) %>%
  readr::read_lines() %>%
  .[1:26] %>%
  trimws() %>%
  str_split_fixed(., "\\.", 2) %>%
  .[,2] %>%
  trimws()
  
met_metadata_text

colnames(met_file1) <- met_metadata_text

met_file1





met_read <- function(file) {
  df <- read_delim(file, delim = "\t")
  
  
}

met_files_full <- map_dfr(met_files, met_read)


summary(met_files_full)
```


Use the `map` function to read in both meteorological files.

```{r}

```


